{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb02abf-673e-4153-ae65-ce0d3c85c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-26 01:22:44 - INFO - HUA-YI-TRAIN-V6.1 - 所有必要套件似乎都已安裝\n",
      "2025-04-26 01:22:44 - INFO - HUA-YI-TRAIN-V6.1 - 設置 TOKENIZERS_PARALLELISM=false\n",
      "2025-04-26 01:22:44 - INFO - HUA-YI-TRAIN-V6.1 - 檢查數據文件是否存在: output/out_cleaned.json\n",
      "2025-04-26 01:22:44 - INFO - HUA-YI-TRAIN-V6.1 - 數據文件找到，準備開始訓練。\n",
      "[rank0]:[W426 01:22:44.191476870 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W426 01:22:44.214595303 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 - --- 開始訓練任務 (v6.1) ---\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   model_name: yentinglin/Llama-3-Taiwan-8B-Instruct\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   data_path: output/out_cleaned.json\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   output_dir: ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   num_train_epochs: 4\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   per_device_batch_size: 1\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   gradient_accumulation_steps: 8\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   learning_rate: 5e-05\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   warmup_ratio: 0.05\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   logging_steps: 10\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   save_steps: 100\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   seed: 42\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   lora_r: 32\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   lora_alpha: 64\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   use_gradient_checkpointing: True\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   gradient_checkpointing_use_reentrant: False\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   dataloader_num_workers: 4\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   report_to: tensorboard\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 -   max_length: 1024\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 - --------------------\n",
      "2025-04-26 01:22:45 - INFO - HUA-YI-TRAIN-V6.1 - 正在從 yentinglin/Llama-3-Taiwan-8B-Instruct 載入基礎模型 (主進程)...\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "2025-04-26 01:22:49 - INFO - HUA-YI-TRAIN-V6.1 - 基礎模型載入成功 (主進程)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "2025-04-26 01:22:50 - INFO - HUA-YI-TRAIN-V6.1 - Tokenizer 使用預設 pad_token (ID: 128001)。\n",
      "2025-04-26 01:22:50 - INFO - HUA-YI-TRAIN-V6.1 - Tokenizer 加載成功. Pad ID: 128001, Padding Side: left\n",
      "2025-04-26 01:22:50 - INFO - HUA-YI-TRAIN-V6.1 - Preparing model (use_gc=True, use_reentrant=False)...\n",
      "2025-04-26 01:22:50 - INFO - HUA-YI-TRAIN-V6.1 - prepare_model_for_kbit_training 完成。\n",
      "2025-04-26 01:22:50 - INFO - HUA-YI-TRAIN-V6.1 - 應用 LoRA 配置: r=32, alpha=64\n",
      "Loading checkpoint shards:  25%|████▌             | 1/4 [00:01<00:03,  1.05s/it]2025-04-26 01:22:51 - INFO - HUA-YI-TRAIN-V6.1 - get_peft_model 完成。\n",
      "trainable params: 83,886,080 || all params: 8,114,163,712 || trainable%: 1.0338\n",
      "2025-04-26 01:22:51 - INFO - HUA-YI-TRAIN-V6.1 - 開始加載和預處理數據從: output/out_cleaned.json\n",
      "2025-04-26 01:22:51 - INFO - HUA-YI-TRAIN-V6.1 - 從 JSON 文件加載: output/out_cleaned.json\n",
      "2025-04-26 01:22:52 - INFO - HUA-YI-TRAIN-V6.1 - 原始數據加載完成: 1439 條樣本\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.17it/s]\n",
      "2025-04-26 01:22:56 - INFO - HUA-YI-TRAIN-V6.1 - Process 0 starting map/filter...\n",
      "2025-04-26 01:22:56 - INFO - HUA-YI-TRAIN-V6.1 - Process 0 preprocessing done. Before filter: 1439, After filter: 1439\n",
      "2025-04-26 01:22:56 - INFO - HUA-YI-TRAIN-V6.1 - 總優化步數: 720, 預熱步數: 36\n",
      "2025-04-26 01:22:56 - INFO - HUA-YI-TRAIN-V6.1 - 使用 Accelerator prepare 包裝組件...\n",
      "2025-04-26 01:22:57 - INFO - HUA-YI-TRAIN-V6.1 - Accelerator prepare 完成\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - **************************************************\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - === 開始訓練 ===\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 -   總樣本數 = 1439\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 -   總批次大小 = 16\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - **************************************************\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - 找到最新的 Checkpoint: ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5/checkpoint-300\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - 開始 trainer.train (resume_from_checkpoint=./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5/checkpoint-300)\n",
      "2025-04-26 01:22:58 - ERROR - HUA-YI-TRAIN-V6.1 - 訓練過程中發生嚴重錯誤: Can't find a checkpoint index (pytorch_model.bin.index.json or model.safetensors.index.json) in ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5/checkpoint-300.\n",
      "Traceback (most recent call last):\n",
      "  File \"/trinity/home/tna001/train.py\", line 251, in train_model\n",
      "    train_result = trainer.train(resume_from_checkpoint=last_checkpoint); logger.info(\"=== 訓練完成 ===\", main_process_only=True)\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/transformers/trainer.py\", line 2217, in train\n",
      "    self._load_from_checkpoint(resume_from_checkpoint)\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/transformers/trainer.py\", line 2896, in _load_from_checkpoint\n",
      "    load_result = load_sharded_checkpoint(\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 470, in load_sharded_checkpoint\n",
      "    raise ValueError(f\"Can't find a checkpoint index ({' or '.join(filenames)}) in {folder}.\")\n",
      "ValueError: Can't find a checkpoint index (pytorch_model.bin.index.json or model.safetensors.index.json) in ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5/checkpoint-300.\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - 清理...\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - GPU緩存已清理\n",
      "2025-04-26 01:22:58 - INFO - HUA-YI-TRAIN-V6.1 - 清理完成\n",
      "2025-04-26 01:22:58 - ERROR - HUA-YI-TRAIN-V6.1 - 配置或數據錯誤: Can't find a checkpoint index (pytorch_model.bin.index.json or model.safetensors.index.json) in ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5/checkpoint-300.\n",
      "[rank0]:[W426 01:22:58.243443864 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0426 01:23:00.097008 509780 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 509824 closing signal SIGTERM\n",
      "E0426 01:23:00.312279 509780 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 509823) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/trinity/home/tna001/.local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
      "    args.func(args)\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 1204, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 825, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/trinity/home/tna001/.local/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-04-26_01:23:00\n",
      "  host      : as1-g-002.cluster-197\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 509823)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 確保 train_huayi_accelerate.py 在當前工作目錄或提供正確相對/絕對路徑\n",
    "!~/.local/bin/accelerate launch train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4705411-99e7-4c37-bf87-0bf1c563ee76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
