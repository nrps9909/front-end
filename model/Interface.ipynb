{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53bc08-cc24-4801-b7c0-aa0f1cc7045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-05-06 16:41:33 HUA-YI-INFERENCE-V6 推理輔助函數已定義 (v5.5 base)。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此腳本設計為在 Jupyter Notebook 中運行。\n",
      "請按順序運行 Cell 來加載模型和啟動對話。\n",
      "1. 運行包含此代碼的 Cell (定義函數)。\n",
      "2. 在新 Cell 中運行: load_model_and_tokenizer()\n",
      "3. 在新 Cell 中運行: start_chat()\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup (導入、配置、加載模型和 Tokenizer)\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time # 用於模擬打字延遲\n",
    "import re  # 引入正則表達式庫用於後處理\n",
    "\n",
    "# --- 設定日誌 ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, # 默認 INFO，start_chat 中會設為 DEBUG\n",
    "    format='[%(levelname)s] %(asctime)s %(name)s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "# --- [修改] 更新 Logger 名稱 ---\n",
    "logger = logging.getLogger(\"HUA-YI-INFERENCE-V6\")\n",
    "\n",
    "# --- 全局變量 ---\n",
    "model = None\n",
    "tokenizer = None\n",
    "is_ready = False\n",
    "\n",
    "# --- 配置 ---\n",
    "base_model_name = \"yentinglin/Llama-3-Taiwan-8B-Instruct\"\n",
    "# --- [關鍵修改] 指向新訓練模型的 Adapter 路徑 ---\n",
    "adapter_path = \"./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5\" # <<<--- 確認這個路徑正確！\n",
    "# --- [建議修改] 更新潛在的合併模型路徑名稱 ---\n",
    "output_merged_model_path = \"./huayi-yenting-llama3-merged-v6-r32-e4-lr5e5\" # <<<--- 建議修改這裡！\n",
    "\n",
    "load_in_4bit = True\n",
    "merge_and_save = False # 在 Notebook 中，除非確定要合併，否則保持 False\n",
    "use_merged_model_if_exists = True # 推薦 True\n",
    "\n",
    "# --- 模型和 Tokenizer 加載函數 (與 v6 推理版本一致) ---\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"加載基礎模型、Adapter 和 Tokenizer (使用 v6 adapter 路徑)\"\"\"\n",
    "    global model, tokenizer, is_ready\n",
    "    if is_ready: logger.info(\"模型和 Tokenizer 已加載。\"); return\n",
    "    logger.info(\"開始加載模型和 Tokenizer...\")\n",
    "    merged_exists = os.path.isdir(output_merged_model_path)\n",
    "    adapter_exists = os.path.isdir(adapter_path)\n",
    "    if not adapter_exists and not merged_exists:\n",
    "        logger.error(f\"LoRA adapter 路徑 ({adapter_path}) 或合併模型路徑 ({output_merged_model_path}) 均未找到\")\n",
    "        raise FileNotFoundError(f\"模型文件未找到，請檢查路徑: {adapter_path} 或 {output_merged_model_path}\")\n",
    "    quant_config = None\n",
    "    model_load_kwargs = {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\", \"trust_remote_code\": True}\n",
    "    if load_in_4bit:\n",
    "        quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\")\n",
    "        model_load_kwargs[\"quantization_config\"] = quant_config\n",
    "        model_load_kwargs[\"low_cpu_mem_usage\"] = True\n",
    "        logger.info(\"使用 4-bit 量化加載模型\")\n",
    "    model_to_load = base_model_name\n",
    "    load_adapter_later = False\n",
    "    tokenizer_load_path = base_model_name\n",
    "    if use_merged_model_if_exists and merged_exists:\n",
    "        logger.info(f\"從已合併模型 {output_merged_model_path} 加載...\")\n",
    "        model_to_load = output_merged_model_path\n",
    "        tokenizer_load_path = output_merged_model_path\n",
    "    elif adapter_exists:\n",
    "        logger.info(f\"從基礎模型 {base_model_name} 和 adapter {adapter_path} 加載...\")\n",
    "        load_adapter_later = True\n",
    "    else: raise FileNotFoundError(\"無法確定有效的模型加載路徑\")\n",
    "    logger.info(f\"正在加載模型: {model_to_load}...\")\n",
    "    try:\n",
    "        import warnings\n",
    "        with warnings.catch_warnings(): warnings.filterwarnings(\"ignore\"); model = AutoModelForCausalLM.from_pretrained(model_to_load, **model_load_kwargs)\n",
    "        logger.info(\"模型加載成功\")\n",
    "    except Exception as e: logger.error(f\"加載模型失敗: {e}\", exc_info=True); raise\n",
    "    if load_adapter_later:\n",
    "        logger.info(f\"正在從 {adapter_path} 加載 LoRA adapter...\")\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(model, adapter_path, is_trainable=False)\n",
    "            logger.info(f\"LoRA adapter 從 {adapter_path} 加載成功\")\n",
    "            if merge_and_save: # 合併邏輯 (merge_and_save=False 時不執行)\n",
    "                logger.warning(\"Merge and Save 設為 True。\"); logger.info(\"合併 LoRA 權重...\")\n",
    "                try:\n",
    "                    model = model.merge_and_unload(); logger.info(\"權重合併完成\")\n",
    "                    if not os.path.exists(output_merged_model_path): os.makedirs(output_merged_model_path)\n",
    "                    logger.info(f\"保存合併模型到 {output_merged_model_path}...\")\n",
    "                    tokenizer_for_save = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
    "                    model.save_pretrained(output_merged_model_path); tokenizer_for_save.save_pretrained(output_merged_model_path)\n",
    "                    logger.info(\"合併模型保存成功\"); tokenizer_load_path = output_merged_model_path\n",
    "                except Exception as e: logger.error(f\"合併或保存失敗: {e}\", exc_info=True); logger.warning(\"繼續使用未合併模型。\"); tokenizer_load_path = base_model_name\n",
    "        except Exception as e: logger.error(f\"加載 LoRA adapter ({adapter_path}) 失敗: {e}\", exc_info=True); raise\n",
    "    logger.info(f\"正在從 {tokenizer_load_path} 加載 Tokenizer...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_load_path, use_fast=True)\n",
    "        if tokenizer.pad_token is None or tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "            logger.warning(f\"Tokenizer PAD token 未設置或等於 EOS。將使用 EOS token (ID: {tokenizer.eos_token_id}) 作為 PAD token。\")\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            if tokenizer.pad_token_id is None: tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            if hasattr(model.config, 'pad_token_id') and model.config.pad_token_id != tokenizer.eos_token_id : model.config.pad_token_id = tokenizer.eos_token_id\n",
    "        else:\n",
    "             if tokenizer.pad_token is None and tokenizer.pad_token_id is not None:\n",
    "                 try: tokenizer.pad_token = tokenizer.decode(tokenizer.pad_token_id)\n",
    "                 except Exception as decode_err: logger.warning(f\"無法解碼 pad_token_id {tokenizer.pad_token_id}: {decode_err}\"); tokenizer.pad_token = f\"<pad:{tokenizer.pad_token_id}>\"\n",
    "             logger.info(f\"Tokenizer 使用預設 PAD token (ID: {tokenizer.pad_token_id})\")\n",
    "        tokenizer.padding_side = 'left'\n",
    "        logger.info(f\"Tokenizer 加載成功. Pad token: '{tokenizer.pad_token}', Pad token ID: {tokenizer.pad_token_id}, Padding side: {tokenizer.padding_side}\")\n",
    "    except Exception as e: logger.error(f\"加載 Tokenizer 失敗: {e}\", exc_info=True); raise\n",
    "    model.eval(); is_ready = True\n",
    "    logger.info(f\"模型 (來自 {model_to_load} + adapter {adapter_path if load_adapter_later else 'N/A'}) 和 Tokenizer 已成功加載並準備就緒！\")\n",
    "\n",
    "# --- 推理函數 (Prompt v5.1 - 修正 f-string 錯誤) ---\n",
    "def create_inference_prompt(persona, goal, conversation_history, last_message):\n",
    "    \"\"\"根據輸入創建推理用的 Prompt, 加入對話歷史, 極端強調風格 (v5.6 - 修正 f-string 錯誤)\"\"\"\n",
    "    prompt_parts = []\n",
    "    # --- 使用與訓練 v6 一致的風格指令 ---\n",
    "    style_instruction = \"\"\"### **[絕對規則] 目標輸出風格 (MUST FOLLOW RULES):**\n",
    "*   **核心要求 (Core Requirement)**: 100% 模仿台灣大學生用 LINE 聊天。輸出**必須極度簡短**！每個概念都必須拆成獨立的短訊息，並且**只能**使用換行符 `\\\\n` 來分隔這些短訊息。\n",
    "*   **絕對禁止 (ABSOLUTELY FORBIDDEN - DO NOT USE):**\n",
    "    *   **任何 Emoji**。 *   **逗號** `,`。 *   **句號** `.` 或 `。`。 *   **引號** `\"` `'` 「」 『』。\n",
    "    *   **頓號** `、`。 *   **分號** `;` 或 `；`。 *   **冒號** `:` 或 `：`。 *   **括號** `()` `（）` `[]`。\n",
    "    *   **任何項目符號或列表標記**。 *   **任何 \"話翼:\" 或類似的角色/機器人名稱前綴**。\n",
    "*   **唯一允許的標點 (ONLY ALLOWED PUNCTUATION):**\n",
    "    *   **空格** ` `。 *   **換行符** `\\\\n`。 *   **問號** `？` (少量)。 *   **驚嘆號** `！` (少量)。\n",
    "*   **訊息長度 (Message Length)**: 總回覆非常簡短。1 到 4 個 `\\\\n` (2 到 5 則短訊息)。\n",
    "*   **輸出格式範例 (Output Format Examples):**\n",
    "    *   範例 1:\\n喔喔\\n原來是這樣\n",
    "    *   範例 2:\\n欸\\n那你想去哪\\n我看看時間\n",
    "    *   範例 3:\\n哈哈\\n真的假的\\n笑死\"\"\"\n",
    "    prompt_parts.append(style_instruction)\n",
    "    if persona: prompt_parts.append(f\"### 對方資訊 (Persona):\\n{persona}\")\n",
    "    if goal: prompt_parts.append(f\"### 你的對話目標 (Your Goal):\\n{goal}\")\n",
    "    context_str = \"\"\n",
    "    if conversation_history and isinstance(conversation_history, list):\n",
    "        history_limit = 6; limited_history = conversation_history[-history_limit:]\n",
    "        formatted_history = []\n",
    "        for entry in limited_history:\n",
    "            if entry.startswith(\"對方：\"): formatted_history.append(entry)\n",
    "            elif entry.startswith(\"話翼：\"):\n",
    "                simplified_reply = entry[3:].strip().replace('\\n', ' ')\n",
    "                formatted_history.append(f\"你：{simplified_reply}\")\n",
    "            else: formatted_history.append(entry)\n",
    "        context_str += \"\\n\".join(formatted_history)\n",
    "    if last_message:\n",
    "        last_msg_clean = str(last_message).replace('\\n', ' ')\n",
    "        if context_str: context_str += f\"\\n對方剛說：{last_msg_clean}\"\n",
    "        else: context_str = f\"對方剛說：{last_msg_clean}\" # 只有 last_message 時\n",
    "\n",
    "    # --- [修正 v5.6] ---\n",
    "    # 處理只有 last_message 沒有 dialogue_context 的情況\n",
    "    if context_str: # 如果 context_str 不為空 (即有 dialogue_context 或 last_message 已被加入)\n",
    "        prompt_parts.append(f\"### 對話紀錄 (Context):\\n{context_str}\")\n",
    "    elif last_message: # 如果 context_str 為空但有 last_message\n",
    "        last_msg_clean_single = str(last_message).replace('\\n', ' ') # 確保單行\n",
    "        prompt_parts.append(f\"### 對方剛說的話:\\n{last_msg_clean_single}\") # 使用修正後的 f-string\n",
    "    # --- 修正結束 ---\n",
    "\n",
    "    prompt_content = \"\\n\\n\".join(prompt_parts)\n",
    "    formatted_prompt = (\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"參考下方所有資訊，並**嚴格遵守**上面列出的**所有絕對規則**，生成一個建議回覆。\\n\\n\"\n",
    "        f\"{prompt_content}\\n\\n\"\n",
    "        f\"**直接生成**完全符合**所有**風格規則的建議回覆：\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    return formatted_prompt\n",
    "\n",
    "# --- 推理函數 (v5.5 base - 參數可調) ---\n",
    "def generate_reply(prompt,\n",
    "                   max_new_tokens=60,\n",
    "                   temperature=0.3,\n",
    "                   top_p=0.7,\n",
    "                   do_sample=True,\n",
    "                   repetition_penalty=1.15,\n",
    "                   no_repeat_ngram_size=3):\n",
    "    \"\"\"使用模型生成回覆建議 (v5.5 base - 參數可調)\"\"\"\n",
    "    global model, tokenizer\n",
    "    if not is_ready: return \"[模型未加載]\"\n",
    "    if not prompt: return \"[Prompt 為空]\"\n",
    "    try:\n",
    "        if not isinstance(prompt, str): logger.error(f\"Prompt type error: {type(prompt)}\"); return \"[Prompt 類型錯誤]\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False, add_special_tokens=False).to(model.device)\n",
    "        if inputs.input_ids.shape[1] == 0: logger.warning(\"Tokenization failed\"); return \"[Tokenization 失敗]\"\n",
    "        if 'attention_mask' not in inputs: inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "        eos_token_ids_set = {tokenizer.eos_token_id}\n",
    "        try:\n",
    "            eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            if eot_id is not None and eot_id != tokenizer.unk_token_id: eos_token_ids_set.add(eot_id); logger.debug(f\"Added <|eot_id|> ({eot_id})\")\n",
    "            elif eot_id == tokenizer.unk_token_id: logger.warning(\"'<|eot_id|>' resolves to UNK\")\n",
    "        except KeyError: logger.warning(\"Tokenizer doesn't know '<|eot_id|>'\")\n",
    "        except Exception as e: logger.error(f\"Error processing '<|eot_id|>' token: {e}\", exc_info=True)\n",
    "        final_eos_ids = list(eos_token_ids_set)\n",
    "        logger.debug(f\"Using EOS token IDs: {final_eos_ids}\")\n",
    "        generate_kwargs = {\n",
    "            \"input_ids\": inputs.input_ids, \"attention_mask\": inputs.attention_mask,\n",
    "            \"max_new_tokens\": max_new_tokens, \"temperature\": temperature, \"top_p\": top_p,\n",
    "            \"do_sample\": do_sample, \"pad_token_id\": tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": final_eos_ids, \"repetition_penalty\": repetition_penalty,\n",
    "            \"no_repeat_ngram_size\": no_repeat_ngram_size,\n",
    "        }\n",
    "        logger.info(f\"開始生成回覆 (temp={temperature}, top_p={top_p}, max_new={max_new_tokens})...\")\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type=model.device.type, dtype=torch.bfloat16): outputs = model.generate(**generate_kwargs)\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        if outputs.shape[1] <= input_length: logger.warning(\"模型未生成任何新的 token。\"); reply = \"\"\n",
    "        else: generated_ids = outputs[0][input_length:]; reply = tokenizer.decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        logger.info(\"生成完成\")\n",
    "        return reply.strip()\n",
    "    except Exception as e: logger.error(f\"生成回覆時出錯: {e}\", exc_info=True); import traceback; logger.error(traceback.format_exc()); return \"[生成錯誤]\"\n",
    "\n",
    "logger.info(\"推理輔助函數已定義 (v5.5 base)。\")\n",
    "\n",
    "# --- 對話狀態管理 ---\n",
    "conversation_history = []\n",
    "# --- 使用與訓練 v6 一致的溫和 Goal ---\n",
    "current_persona = \"25歲正在就讀科技法律研究所的女生，大學是吉他社，很喜歡日本文化，安全型人格，住在台北，討厭把家庭順位放太高的男生，討厭父權男，不喜歡在小事情上計較\"\n",
    "current_goal = \"目前我跟這個女生是朋友，我想要跟他變成更好的朋友\" # <<<--- 保持溫和目標\n",
    "\n",
    "# --- 交互式對話循環 (v5.4 base - 使用 v5.5 generate_reply, v5.3 後處理) ---\n",
    "def start_chat():\n",
    "    \"\"\"啟動交互式對話循環 (使用 v6 模型)\"\"\"\n",
    "    global conversation_history\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.warning(\"Logger level set to DEBUG for this chat session.\")\n",
    "    # --- [修改] 更新版本號 ---\n",
    "    print(\"\\n=========================================\")\n",
    "    print(\"  話翼 AI 社交教練 - Jupyter 對話模式 v6 (fix 5.6)\") # 更新版本號\n",
    "    print(f\"    (Adapter: {adapter_path})\")\n",
    "    print(\"=========================================\")\n",
    "    print(\"\\n指令: quit, reset, history\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"[Persona: {current_persona}]\")\n",
    "    print(f\"[Goal: {current_goal}]\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # --- 後處理正則 (與 v5.3/v5.4 一致，移除了 Emoji 過濾) ---\n",
    "    prefix_pattern = re.compile(r'^\\s*(?:話翼|AI|Assistant|模型|回答|建議)\\s*[:：]?\\s*', re.IGNORECASE)\n",
    "    forbidden_punctuation_list = \",.，。、；：「」『』（）《》\\\"'();:\" # 保持移除這些\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\" * 20)\n",
    "            print(f\"[歷史記錄長度: {len(conversation_history)}]\")\n",
    "            try: user_input = input(\">>> 你 (對方剛說的話 或 指令): \").strip()\n",
    "            except (EOFError, KeyboardInterrupt): print(\"\\n檢測到退出信號...\"); user_input = \"quit\"\n",
    "            if user_input.lower() == 'quit': print(\"\\n正在退出...\"); break\n",
    "            elif user_input.lower() == 'reset':\n",
    "                conversation_history = []; logger.info(\"對話已重置\")\n",
    "                print(\"\\n[對話已重置]\\n\"); print(\"-\" * 40); print(f\"[Persona: {current_persona}]\"); print(f\"[Goal: {current_goal}]\"); print(\"-\" * 40); continue\n",
    "            elif user_input.lower() == 'history':\n",
    "                 print(\"\\n--- 對話歷史 ---\");\n",
    "                 if conversation_history: [print(f\"{i+1}: {msg}\") for i, msg in enumerate(conversation_history)]\n",
    "                 else: print(\"(空)\")\n",
    "                 print(\"----------------\"); continue\n",
    "            if not user_input: print(\"  [提示] 請輸入對方說的話...\"); continue\n",
    "\n",
    "            # 使用 v5.6 Prompt (修正了 f-string 錯誤)\n",
    "            prompt = create_inference_prompt(persona=current_persona, goal=current_goal, conversation_history=conversation_history, last_message=user_input)\n",
    "            logger.debug(f\"Generated Prompt (first 500 chars):\\n---\\n{prompt[:500]}...\\n---\")\n",
    "\n",
    "            # 使用 v5.5 generate_reply\n",
    "            raw_ai_reply = generate_reply(prompt=prompt)\n",
    "            logger.debug(f\"Raw AI Reply: '{raw_ai_reply}'\")\n",
    "\n",
    "            # --- 後處理步驟 (v5.4 邏輯) ---\n",
    "            cleaned_reply_step1 = prefix_pattern.sub('', raw_ai_reply).strip(); logger.debug(f\"After Step 1 (Prefix): '{cleaned_reply_step1}'\")\n",
    "            logger.debug(\"Skipping Step 2 (Emoji)\")\n",
    "            cleaned_reply_step3 = cleaned_reply_step1; removed_punctuations = []\n",
    "            for punc in forbidden_punctuation_list:\n",
    "                if punc in cleaned_reply_step3: removed_punctuations.append(punc); cleaned_reply_step3 = cleaned_reply_step3.replace(punc, '')\n",
    "            if removed_punctuations: logger.debug(f\"Removed punctuations: {' '.join(removed_punctuations)}\")\n",
    "            logger.debug(f\"After Step 3 (Punctuation): '{cleaned_reply_step3}'\")\n",
    "\n",
    "            cleaned_reply_step4 = cleaned_reply_step3.replace('\\\\n', '\\n')\n",
    "            logger.debug(f\"After Step 4 (Newline Replace): '{cleaned_reply_step4}'\")\n",
    "\n",
    "            processed_reply = cleaned_reply_step4\n",
    "\n",
    "            lines = processed_reply.split('\\n')\n",
    "            cleaned_lines = []\n",
    "            logger.debug(f\"Processing lines (count={len(lines)}):\")\n",
    "            for i, line in enumerate(lines):\n",
    "                stripped_line = line.strip()\n",
    "                logger.debug(f\"  Line {i}: Raw='{line}' -> Stripped='{stripped_line}'\")\n",
    "                if stripped_line: cleaned_lines.append(stripped_line)\n",
    "                else: logger.debug(f\"  Line {i} was empty after stripping, discarded.\")\n",
    "\n",
    "            final_reply = \"\\n\".join(cleaned_lines)\n",
    "            logger.debug(f\"Final Reply after joining (num lines={len(cleaned_lines)}): '{final_reply}'\")\n",
    "\n",
    "            # --- 打印處理後的回覆 ---\n",
    "            print(f\"\\n✨ 話翼建議:\")\n",
    "            if not final_reply:\n",
    "                print(\"(模型未生成有效回覆 或 回覆被過濾)\")\n",
    "                logger.warning(f\"最終回覆為空。原始回覆: '{raw_ai_reply}'\")\n",
    "                final_reply_for_history = \"[過濾後為空]\"\n",
    "            else:\n",
    "                lines_to_print = final_reply.split('\\n')\n",
    "                for i, line in enumerate(lines_to_print): print(line); time.sleep(0.2) if i < len(lines_to_print) - 1 else None\n",
    "                final_reply_for_history = final_reply\n",
    "\n",
    "            # 更新對話歷史\n",
    "            conversation_history.append(f\"對方：{user_input}\"); conversation_history.append(f\"話翼：{final_reply_for_history}\")\n",
    "            history_limit = 8\n",
    "            if len(conversation_history) > history_limit: conversation_history = conversation_history[-history_limit:]\n",
    "\n",
    "        except Exception as e: logger.error(f\"對話循環中發生錯誤: {e}\", exc_info=True); print(\"\\n發生內部錯誤，請檢查日誌。\")\n",
    "\n",
    "    logger.setLevel(logging.INFO); logger.info(\"Chat session ended. Logger level reset to INFO.\")\n",
    "    print(\"\\n對話結束！\")\n",
    "\n",
    "# --- 主執行區塊 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"此腳本設計為在 Jupyter Notebook 中運行。\")\n",
    "    print(\"請按順序運行 Cell 來加載模型和啟動對話。\")\n",
    "    print(\"1. 運行包含此代碼的 Cell (定義函數)。\")\n",
    "    print(\"2. 在新 Cell 中運行: load_model_and_tokenizer()\")\n",
    "    print(\"3. 在新 Cell 中運行: start_chat()\")\n",
    "\n",
    "# --- 使用說明 ---\n",
    "# ... (同上)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5c14b-7def-4dc8-a7c4-a7d10261e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-05-06 16:41:33 HUA-YI-INFERENCE-V6 開始加載模型和 Tokenizer...\n",
      "[INFO] 2025-05-06 16:41:33 HUA-YI-INFERENCE-V6 使用 4-bit 量化加載模型\n",
      "[INFO] 2025-05-06 16:41:33 HUA-YI-INFERENCE-V6 從基礎模型 yentinglin/Llama-3-Taiwan-8B-Instruct 和 adapter ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5 加載...\n",
      "[INFO] 2025-05-06 16:41:33 HUA-YI-INFERENCE-V6 正在加載模型: yentinglin/Llama-3-Taiwan-8B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738a1b598e56494abb47db769a1aeffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-05-06 16:44:04 HUA-YI-INFERENCE-V6 模型加載成功\n",
      "[INFO] 2025-05-06 16:44:04 HUA-YI-INFERENCE-V6 正在從 ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5 加載 LoRA adapter...\n",
      "/trinity/home/tna001/.local/lib/python3.9/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n",
      "[INFO] 2025-05-06 16:44:08 HUA-YI-INFERENCE-V6 LoRA adapter 從 ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5 加載成功\n",
      "[INFO] 2025-05-06 16:44:08 HUA-YI-INFERENCE-V6 正在從 yentinglin/Llama-3-Taiwan-8B-Instruct 加載 Tokenizer...\n",
      "[INFO] 2025-05-06 16:44:09 HUA-YI-INFERENCE-V6 Tokenizer 使用預設 PAD token (ID: 128001)\n",
      "[INFO] 2025-05-06 16:44:09 HUA-YI-INFERENCE-V6 Tokenizer 加載成功. Pad token: '<|end_of_text|>', Pad token ID: 128001, Padding side: left\n",
      "[INFO] 2025-05-06 16:44:09 HUA-YI-INFERENCE-V6 模型 (來自 yentinglin/Llama-3-Taiwan-8B-Instruct + adapter ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5) 和 Tokenizer 已成功加載並準備就緒！\n",
      "[WARNING] 2025-05-06 16:44:09 HUA-YI-INFERENCE-V6 Logger level set to DEBUG for this chat session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "  話翼 AI 社交教練 - Jupyter 對話模式 v6 (fix 5.6)\n",
      "    (Adapter: ./huayi-yenting-llama3-lora-accelerate-v6-r32-e4-lr5e5)\n",
      "=========================================\n",
      "\n",
      "指令: quit, reset, history\n",
      "----------------------------------------\n",
      "[Persona: 25歲正在就讀科技法律研究所的女生，大學是吉他社，很喜歡日本文化，安全型人格，住在台北，討厭把家庭順位放太高的男生，討厭父權男，不喜歡在小事情上計較]\n",
      "[Goal: 目前我跟這個女生是朋友，我想要跟他變成更好的朋友]\n",
      "----------------------------------------\n",
      "\n",
      "====================\n",
      "[歷史記錄長度: 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 你 (對方剛說的話 或 指令):  早安\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2025-05-06 16:44:14 HUA-YI-INFERENCE-V6 Generated Prompt (first 500 chars):\n",
      "---\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "參考下方所有資訊，並**嚴格遵守**上面列出的**所有絕對規則**，生成一個建議回覆。\n",
      "\n",
      "### **[絕對規則] 目標輸出風格 (MUST FOLLOW RULES):**\n",
      "*   **核心要求 (Core Requirement)**: 100% 模仿台灣大學生用 LINE 聊天。輸出**必須極度簡短**！每個概念都必須拆成獨立的短訊息，並且**只能**使用換行符 `\\n` 來分隔這些短訊息。\n",
      "*   **絕對禁止 (ABSOLUTELY FORBIDDEN - DO NOT USE):**\n",
      "    *   **任何 Emoji**。 *   **逗號** `,`。 *   **句號** `.` 或 `。`。 *   **引號** `\"` `'` 「」 『』。\n",
      "    *   **頓號** `、`。 *   **分號** `;` 或 `；`。 *   **冒號** `:` 或 `：`。 *   **括號** `()` `（）` `[]`。\n",
      "    *   **任何項...\n",
      "---\n",
      "[DEBUG] 2025-05-06 16:44:14 HUA-YI-INFERENCE-V6 Added <|eot_id|> (128009)\n",
      "[DEBUG] 2025-05-06 16:44:14 HUA-YI-INFERENCE-V6 Using EOS token IDs: [128009]\n",
      "[INFO] 2025-05-06 16:44:14 HUA-YI-INFERENCE-V6 開始生成回覆 (temp=0.3, top_p=0.7, max_new=60)...\n",
      "[INFO] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 生成完成\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 Raw AI Reply: '嗨\n",
      "今天過得好嗎'\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 After Step 1 (Prefix): '嗨\n",
      "今天過得好嗎'\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 Skipping Step 2 (Emoji)\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 After Step 3 (Punctuation): '嗨\n",
      "今天過得好嗎'\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 After Step 4 (Newline Replace): '嗨\n",
      "今天過得好嗎'\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 Processing lines (count=2):\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6   Line 0: Raw='嗨' -> Stripped='嗨'\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6   Line 1: Raw='今天過得好嗎' -> Stripped='今天過得好嗎'\n",
      "[DEBUG] 2025-05-06 16:44:19 HUA-YI-INFERENCE-V6 Final Reply after joining (num lines=2): '嗨\n",
      "今天過得好嗎'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ 話翼建議:\n",
      "嗨\n",
      "今天過得好嗎\n",
      "\n",
      "====================\n",
      "[歷史記錄長度: 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 你 (對方剛說的話 或 指令):  我今天在面試醫療產業的科技公司 你覺得如何\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2025-05-06 16:44:44 HUA-YI-INFERENCE-V6 Generated Prompt (first 500 chars):\n",
      "---\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "參考下方所有資訊，並**嚴格遵守**上面列出的**所有絕對規則**，生成一個建議回覆。\n",
      "\n",
      "### **[絕對規則] 目標輸出風格 (MUST FOLLOW RULES):**\n",
      "*   **核心要求 (Core Requirement)**: 100% 模仿台灣大學生用 LINE 聊天。輸出**必須極度簡短**！每個概念都必須拆成獨立的短訊息，並且**只能**使用換行符 `\\n` 來分隔這些短訊息。\n",
      "*   **絕對禁止 (ABSOLUTELY FORBIDDEN - DO NOT USE):**\n",
      "    *   **任何 Emoji**。 *   **逗號** `,`。 *   **句號** `.` 或 `。`。 *   **引號** `\"` `'` 「」 『』。\n",
      "    *   **頓號** `、`。 *   **分號** `;` 或 `；`。 *   **冒號** `:` 或 `：`。 *   **括號** `()` `（）` `[]`。\n",
      "    *   **任何項...\n",
      "---\n",
      "[DEBUG] 2025-05-06 16:44:44 HUA-YI-INFERENCE-V6 Added <|eot_id|> (128009)\n",
      "[DEBUG] 2025-05-06 16:44:44 HUA-YI-INFERENCE-V6 Using EOS token IDs: [128009]\n",
      "[INFO] 2025-05-06 16:44:44 HUA-YI-INFERENCE-V6 開始生成回覆 (temp=0.3, top_p=0.7, max_new=60)...\n",
      "[INFO] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 生成完成\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 Raw AI Reply: '欸 原來妳在做什麼工作啊 我以為妳唸法律系的'\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 After Step 1 (Prefix): '欸 原來妳在做什麼工作啊 我以為妳唸法律系的'\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 Skipping Step 2 (Emoji)\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 After Step 3 (Punctuation): '欸 原來妳在做什麼工作啊 我以為妳唸法律系的'\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 After Step 4 (Newline Replace): '欸 原來妳在做什麼工作啊 我以為妳唸法律系的'\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 Processing lines (count=1):\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6   Line 0: Raw='欸 原來妳在做什麼工作啊 我以為妳唸法律系的' -> Stripped='欸 原來妳在做什麼工作啊 我以為妳唸法律系的'\n",
      "[DEBUG] 2025-05-06 16:44:48 HUA-YI-INFERENCE-V6 Final Reply after joining (num lines=1): '欸 原來妳在做什麼工作啊 我以為妳唸法律系的'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ 話翼建議:\n",
      "欸 原來妳在做什麼工作啊 我以為妳唸法律系的\n",
      "\n",
      "====================\n",
      "[歷史記錄長度: 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 你 (對方剛說的話 或 指令):  我在做老師\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2025-05-06 16:45:29 HUA-YI-INFERENCE-V6 Generated Prompt (first 500 chars):\n",
      "---\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "參考下方所有資訊，並**嚴格遵守**上面列出的**所有絕對規則**，生成一個建議回覆。\n",
      "\n",
      "### **[絕對規則] 目標輸出風格 (MUST FOLLOW RULES):**\n",
      "*   **核心要求 (Core Requirement)**: 100% 模仿台灣大學生用 LINE 聊天。輸出**必須極度簡短**！每個概念都必須拆成獨立的短訊息，並且**只能**使用換行符 `\\n` 來分隔這些短訊息。\n",
      "*   **絕對禁止 (ABSOLUTELY FORBIDDEN - DO NOT USE):**\n",
      "    *   **任何 Emoji**。 *   **逗號** `,`。 *   **句號** `.` 或 `。`。 *   **引號** `\"` `'` 「」 『』。\n",
      "    *   **頓號** `、`。 *   **分號** `;` 或 `；`。 *   **冒號** `:` 或 `：`。 *   **括號** `()` `（）` `[]`。\n",
      "    *   **任何項...\n",
      "---\n",
      "[DEBUG] 2025-05-06 16:45:29 HUA-YI-INFERENCE-V6 Added <|eot_id|> (128009)\n",
      "[DEBUG] 2025-05-06 16:45:29 HUA-YI-INFERENCE-V6 Using EOS token IDs: [128009]\n",
      "[INFO] 2025-05-06 16:45:29 HUA-YI-INFERENCE-V6 開始生成回覆 (temp=0.3, top_p=0.7, max_new=60)...\n",
      "[INFO] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 生成完成\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 Raw AI Reply: '欸 老師 真酷耶'\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 After Step 1 (Prefix): '欸 老師 真酷耶'\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 Skipping Step 2 (Emoji)\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 After Step 3 (Punctuation): '欸 老師 真酷耶'\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 After Step 4 (Newline Replace): '欸 老師 真酷耶'\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 Processing lines (count=1):\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6   Line 0: Raw='欸 老師 真酷耶' -> Stripped='欸 老師 真酷耶'\n",
      "[DEBUG] 2025-05-06 16:45:31 HUA-YI-INFERENCE-V6 Final Reply after joining (num lines=1): '欸 老師 真酷耶'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ 話翼建議:\n",
      "欸 老師 真酷耶\n",
      "\n",
      "====================\n",
      "[歷史記錄長度: 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 你 (對方剛說的話 或 指令):  那你呢\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2025-05-06 16:45:54 HUA-YI-INFERENCE-V6 Generated Prompt (first 500 chars):\n",
      "---\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "參考下方所有資訊，並**嚴格遵守**上面列出的**所有絕對規則**，生成一個建議回覆。\n",
      "\n",
      "### **[絕對規則] 目標輸出風格 (MUST FOLLOW RULES):**\n",
      "*   **核心要求 (Core Requirement)**: 100% 模仿台灣大學生用 LINE 聊天。輸出**必須極度簡短**！每個概念都必須拆成獨立的短訊息，並且**只能**使用換行符 `\\n` 來分隔這些短訊息。\n",
      "*   **絕對禁止 (ABSOLUTELY FORBIDDEN - DO NOT USE):**\n",
      "    *   **任何 Emoji**。 *   **逗號** `,`。 *   **句號** `.` 或 `。`。 *   **引號** `\"` `'` 「」 『』。\n",
      "    *   **頓號** `、`。 *   **分號** `;` 或 `；`。 *   **冒號** `:` 或 `：`。 *   **括號** `()` `（）` `[]`。\n",
      "    *   **任何項...\n",
      "---\n",
      "[DEBUG] 2025-05-06 16:45:54 HUA-YI-INFERENCE-V6 Added <|eot_id|> (128009)\n",
      "[DEBUG] 2025-05-06 16:45:54 HUA-YI-INFERENCE-V6 Using EOS token IDs: [128009]\n",
      "[INFO] 2025-05-06 16:45:54 HUA-YI-INFERENCE-V6 開始生成回覆 (temp=0.3, top_p=0.7, max_new=60)...\n",
      "[INFO] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 生成完成\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 Raw AI Reply: '欸 那你現在在幹嘛\n",
      "我在準備期中考試\n",
      "還沒畢業耶'\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 After Step 1 (Prefix): '欸 那你現在在幹嘛\n",
      "我在準備期中考試\n",
      "還沒畢業耶'\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 Skipping Step 2 (Emoji)\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 After Step 3 (Punctuation): '欸 那你現在在幹嘛\n",
      "我在準備期中考試\n",
      "還沒畢業耶'\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 After Step 4 (Newline Replace): '欸 那你現在在幹嘛\n",
      "我在準備期中考試\n",
      "還沒畢業耶'\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 Processing lines (count=3):\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6   Line 0: Raw='欸 那你現在在幹嘛' -> Stripped='欸 那你現在在幹嘛'\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6   Line 1: Raw='我在準備期中考試' -> Stripped='我在準備期中考試'\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6   Line 2: Raw='還沒畢業耶' -> Stripped='還沒畢業耶'\n",
      "[DEBUG] 2025-05-06 16:45:58 HUA-YI-INFERENCE-V6 Final Reply after joining (num lines=3): '欸 那你現在在幹嘛\n",
      "我在準備期中考試\n",
      "還沒畢業耶'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ 話翼建議:\n",
      "欸 那你現在在幹嘛\n",
      "我在準備期中考試\n",
      "還沒畢業耶\n",
      "\n",
      "====================\n",
      "[歷史記錄長度: 8]\n"
     ]
    }
   ],
   "source": [
    "load_model_and_tokenizer()\n",
    "start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a275458-4ccb-4e02-a093-70496ef3de55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
